{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking WebScraper # "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebooks contains the code to scrape the information off of some of Booking.com's Portugal accomodations.\n",
    "\n",
    "This webscraper utilyzes the Selinum Framwork and the BS4 Library.<br>\n",
    "We are using selenium beacuse booking.com's webpage is dynamicly loaded meaning we have to load the page to a brower to get the correct and complete html code. A simple http request using the request library is not enought to get the full html.<br>\n",
    "We are using the BS4 to easiy scrape the data on the extracted html.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Importing all Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame, read_excel, concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the options ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webdriver laod options\n",
    "options = Options()\n",
    "options.headless = False # If False this will open the webdriver window. If True the scraper will operate in headless mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identifying the base URL to scrape the Top Destinations of Booking ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note ###\n",
    "\n",
    "Portugal has over 40K listings on Booking. There is no way to access to all listing since booking only loads up to 40 pages at a time (1000 listings.) <br>\n",
    "Therefore, we decided to get only the data of Booking's Top Destinations' cities. If a city has more than 1000 listing we will only get the first 1000 that booking displays to us. <br>\n",
    "Only 6 cities have more than 1000 accomodations listed on Booking: Lisbon, Porto, Albufeira, Portimão, Lagos and Funchal.\n",
    "\n",
    "We identifyed the base URL that only changes based on the Top destiantions chosen. We will scrape the top destinations to then append to the base URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.booking.com/searchresults.en-us.html?ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAK27smiBsACAdICJDAzNmNmODZkLTEzNWUtNGY4YS05NDRjLTA4NDNhOTcyYTJlZNgCBeACAQ&aid=304142&lang=en-us&sb=1&src_elem=sb&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrapin the top destinations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_14944\\1726225282.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n"
     ]
    }
   ],
   "source": [
    "# Loading the Driver\n",
    "driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n",
    "driver.get(url)\n",
    "time.sleep(5) # These seconds  allow the page to fully load up.\n",
    "\n",
    "# Get the Page Source to parse it with BS4\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Closing the Driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new BS4 object with the new page source\n",
    "soup = BeautifulSoup(page_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Top destinations divisions from the HTML as a list.\n",
    "dest_div = soup.find(\"div\", class_ =\"ffa9856b86 c9c3c66a7d f7d4209a37\").find(\"div\", id = \"filter_group_top_destinations_:R3cq:\").find_all(\"div\", class_ = \"db29ecfbe2 c072c8cf10 c3f5dab487 a244b31530 a2d2507aff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicintionary to place all the top destinations and their specific code.\n",
    "destinations = {}\n",
    "\n",
    "# Looping all the top division to get their name and code\n",
    "for dest in dest_div:\n",
    "    destinations[dest.find(\"input\").get(\"aria-label\").split(\":\",1)[0]] = dest.find(\"input\").get(\"name\").split(\"=\",1)[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scraping the top 25 cities of the top destinations ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code from the top destinations, we can loop the extended the url below with therespective code of the Top Destination. <br>\n",
    "After it, we will scrape the cities and their respective code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_url = \"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=top_destinations%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_14944\\2113657281.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  cities_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n"
     ]
    }
   ],
   "source": [
    "cities = {} # City dict to put the top region and the cities code \n",
    "cities_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n",
    "for top in destinations:\n",
    "    cities_driver.get(dest_url + str(destinations[top]))\n",
    "    cities[top] = [] # Empty list to put the city and code dict\n",
    "    time.sleep(10) # allow the page to fully load up.\n",
    "    cities_page_source = cities_driver.page_source # Get the Page Source to parse it with BS4\n",
    "    soup_cities = BeautifulSoup(cities_page_source, \"lxml\") # Create a new BS4 object with the new page source\n",
    "    cities_div = soup_cities.find(\"div\", class_ =\"ffa9856b86 c9c3c66a7d f7d4209a37\").find(\"div\", id = \"filter_group_uf_:R34q:\").find_all(\"div\", class_ = \"db29ecfbe2 c072c8cf10 c3f5dab487 a244b31530 a2d2507aff\") # Get the Cities divisions from the HTML as a list.\n",
    "    for cit in cities_div:\n",
    "        city = {}\n",
    "        city [cit.find(\"input\").get(\"aria-label\").split(\":\",1)[0]] = cit.find(\"input\").get(\"name\").split(\"=\",1)[1] # Get the city's code\n",
    "        cities[top].append(city)\n",
    "cities_driver.quit() # Closing the Driver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Dataframe with all the Top destinations and cities Combinations ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the top destinations and city code, we will create a dataframe with that information and the respective URL. <br>\n",
    "We will then save that information to an XLSX file, so we don't have to scrape the infromation again, we can simply read a file with the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to be formated below\n",
    "# \"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=top_destinations%3D%3Buf%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [] #Empty list to append all dict. This will later be coverted to a dataframe\n",
    "\n",
    "for top, city_list in cities.items(): # Looping all the top destinations\n",
    "    for cidade in city_list:    # Looping all the cities in the top destinations\n",
    "        url_dict = {}   # Dict will have the top destination, city and repective url. The URL below is a formatted string with the repective top destination and city code.\n",
    "        url_dict[\"Top Destination\"] = top # Top destiantion\n",
    "        url_dict[\"City\"] = list(cidade.keys())[0] # City\n",
    "        # Add the respective URL city\n",
    "        url_dict[\"Url\"] = f\"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=top_destinations%3D{destinations[top]}%3Buf%3D{cidade[list(cidade.keys())[0]]}\"\n",
    "        urls.append(url_dict) # Append the cities URL to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = DataFrame(urls) # DF with the information Above\n",
    "df_cities.to_excel(\"Urls/Destination and City URLs.xlsx\", index=False) # Saving the DF to a Xlsx file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scraping the information of each Top Destination/City URL ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note ###\n",
    "\n",
    "Steps 3 to 6 can be skipped, and instead we can read the following XLSX file \"Scraped Data/Destination and City URLs.xlsx\". <br>\n",
    "This file contains the URLs of all the pages to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = read_excel(\"Urls/Destination and City URLs.xlsx\") # Readt th DF\n",
    "dic_cities = df_read.to_dict('records') # Change the DF to a list of Dictionaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the URL Below is the base URL for a City.<br>\n",
    "Booking Website has a max of 40 pages displayed at one time. Each with 25 accomodations. <br>\n",
    "Each page has a code ate the URL \"offset=\". <br>\n",
    "We will loop through all fo the pages by changing the URL and get the summary information of the accomodations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=top_destinations%3D%3Buf%3D-&offset=\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function:** get_summary_info(soup_object) ###\n",
    "\n",
    "The function below will retrives the Name, the URL the Rating, the Revew Score, the number of reviews and the calssification fo the accomodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_info(soup_object):\n",
    "    summary = {}\n",
    "    try:\n",
    "        summary[\"Name\"] = soup_object.find(\"div\", class_ = \"fcab3ed991 a23c043802\").text # Get the name of the accomodation\n",
    "    except:\n",
    "         summary[\"Name\"] = \"\"\n",
    "    try:\n",
    "        summary[\"URL\"] = soup_object.find(\"div\", class_ = \"d20f4628d0\").find(\"div\", class_ = \"f9d4f2568d\").find(\"div\", class_ = \"c90a25d457\").find(\"a\").get(\"href\") # Get the URL of the accomodation\n",
    "    except:\n",
    "        summary[\"URL\"] = \"\"\n",
    "    try:\n",
    "        summary[\"Rating\"] = soup_object.find(\"div\", class_ = \"f919b8b3d5\").find(\"div\", class_ = \"e4755bbd60\").get(\"aria-label\") # Get the rating the accomodation\n",
    "    except:\n",
    "        summary[\"Rating\"] = \"\"\n",
    "    try:\n",
    "        summary[\"Review_Score\"] = soup_object.find_all(\"div\", class_ = \"a1b3f50dcd ef8295f3e6 f7c6687c3d\")[1].find(\"span\").find(\"div\", class_ = \"b5cd09854e d10a6220b4\").text # Get the review score of the accomodation\n",
    "    except:\n",
    "        summary[\"Review_Score\"] = \"\"\n",
    "    try:\n",
    "        summary[\"Number_Reviews\"] =  soup_object.find_all(\"div\", class_ = \"a1b3f50dcd ef8295f3e6 f7c6687c3d\")[1].find(\"span\").find(\"div\", class_ = \"d8eab2cf7f c90c0a70d3 db63693c62\").text # Get the review score of the accomodation\n",
    "    except:\n",
    "        summary[\"Number_Reviews\"] = \"\"\n",
    "    try:\n",
    "        summary[\"Classification\"] =  soup_object.find_all(\"div\", class_ = \"a1b3f50dcd ef8295f3e6 f7c6687c3d\")[1].find(\"span\").find(\"div\", class_ = \"b5cd09854e f0d4d6a2f5 e46e88563a\").get(\"aria-label\").strip() # Get the classification score of the accomodation\n",
    "    except:\n",
    "        summary[\"Classification\"] = \"\"\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_14944\\1483894870.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  city_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n"
     ]
    }
   ],
   "source": [
    "# First Part we will get the number of pages per city we have available\n",
    "city_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n",
    "for city in dic_cities:\n",
    "    city_driver.get(city[\"Url\"])\n",
    "    time.sleep(5) # allow the page to fully load up.\n",
    "    city_page_source = city_driver.page_source # Get the Page Source to parse it with BS4\n",
    "    soup_city = BeautifulSoup(city_page_source, \"lxml\") # Create a new BS4 object with the new page source\n",
    "    # This try and except, is only in case a city has less than 25 accomodations and only has one page.\n",
    "    try:\n",
    "        last_page = soup_city.find(\"div\", class_=\"d7a0553560\").find(\"ol\", class_ = \"a8b500abde\").find_all(\"li\", class_=\"f32a99c8d1\")[-1].find(\"button\").get(\"aria-label\").strip() # This gets the number of the last page.\n",
    "        city_places = [] # Dict to append the accomodations of the city\n",
    "        for page in list(range(0,int(last_page)*25,25)): # THis will loop all the pages of the city\n",
    "            city_driver.get(city[\"Url\"]+\"&offset=\"+str(page)) # Go to the page\n",
    "            time.sleep(5) # load pgae\n",
    "            city_page_source = city_driver.page_source # get HTML\n",
    "            soup_city = BeautifulSoup(city_page_source, \"lxml\") #Parse HTML\n",
    "            houses = [] # List to add the accomodations\n",
    "            for house in soup_city.find_all(\"div\", class_ = \"a826ba81c4 fe821aea6c fa2f36ad22 afd256fc79 d08f526e0d ed11e24d01 ef9845d4b3 da89aeb942\"): # loop all the accomodations in a page\n",
    "                place = {} #dict to put the info\n",
    "                place[\"Top_destination\"] = city[\"Top Destination\"] # Top dest\n",
    "                place[\"City\"] = city[\"City\"] # City\n",
    "                place_2 = get_summary_info(house) # Apply function described above\n",
    "                final_place = place | place_2 # merge the 2 dict\n",
    "                houses.append(final_place) # append final dic to the list\n",
    "            city_places.extend(houses)\n",
    "        df_cityplaces = DataFrame(city_places) #convert list of dict to DF\n",
    "        df_cityplaces.to_excel(f\"Scraped Data/Top- {city['Top Destination']} City- {city['City']}.xlsx\", index=False) # Save to excel file\n",
    "    # The except below works the exact way as the try above but only for the first page. This conditionwill only be ran if the city has less than 1 page\n",
    "    except:\n",
    "        city_places = []\n",
    "        city_driver.get(city[\"Url\"]+\"&offset=\"+str(0))\n",
    "        time.sleep(5)\n",
    "        soup_city = BeautifulSoup(city_page_source, \"lxml\")\n",
    "        houses = []\n",
    "        for house in soup_city.find_all(\"div\", class_ = \"a826ba81c4 fe821aea6c fa2f36ad22 afd256fc79 d08f526e0d ed11e24d01 ef9845d4b3 da89aeb942\"):\n",
    "            place = {}\n",
    "            place[\"Top_destination\"] = city[\"Top Destination\"]\n",
    "            place[\"City\"] = city[\"City\"]\n",
    "            place_2 = get_summary_info(house)\n",
    "            final_place = place | place_2\n",
    "            houses.append(final_place)\n",
    "            city_places.extend(houses)\n",
    "        df_cityplaces = DataFrame(city_places)\n",
    "        df_cityplaces.to_excel(f\"Scraped Data/Top- {city['Top Destination']} City- {city['City']}.xlsx\", index=False)\n",
    "city_driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Getting other top 25 cities on booking not in the top destinations ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cities, like Lisbon, are not included in any of the top destinations.\n",
    "We will get the total top 25 citties and then see what cities are not in any top destination. <br>\n",
    "After it we will scrape the info of those cities using the code written in the step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_17112\\3917965545.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n"
     ]
    }
   ],
   "source": [
    "# Loading the Driver\n",
    "driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n",
    "driver.get(url)\n",
    "time.sleep(10) # allow the page to fully load up.\n",
    "\n",
    "# Get the Page Source to parse it with BS4\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Closing the Driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new BS4 object with the new page source\n",
    "others_soup = BeautifulSoup(page_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Top destinations divisions from the HTML as a list.\n",
    "other_cities = others_soup.find(\"div\", class_ =\"ffa9856b86 c9c3c66a7d f7d4209a37\").find(\"div\", id = \"filter_group_uf_:R34q:\").find_all(\"div\", class_ = \"db29ecfbe2 c072c8cf10 c3f5dab487 a244b31530 a2d2507aff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicintionary to place all the top destinations and their specific code.\n",
    "oth_city = {}\n",
    "\n",
    "# Looping all the top division to get their name and code\n",
    "for cit in other_cities:\n",
    "        oth_city[cit.find(\"input\").get(\"aria-label\").split(\":\",1)[0]] = cit.find(\"input\").get(\"name\").split(\"=\",1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_urls = []\n",
    "for other, cyti in oth_city.items(): # Looping all the top destinations\n",
    "    url_dict = {}   # Dict will have the top destination, city and repective url. The URL below is a formatted string with the repective top destination and city code.\n",
    "    url_dict[\"Top Destination\"] = \"Other\"\n",
    "    url_dict[\"City\"] = other\n",
    "    url_dict[\"Url\"] = f\"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=uf%3D{str(cyti)}\"\n",
    "    other_urls.append(url_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the other_urls to remove the cities that are in any of the top destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = read_excel(\"Urls/Destination and City URLs.xlsx\")\n",
    "dic_cities = df_read.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for index, dic in enumerate(other_urls):\n",
    "    for city in dic_cities:\n",
    "        if dic[\"City\"] == city[\"City\"]:\n",
    "            try:\n",
    "                indexes.append(index)\n",
    "            except:\n",
    "                None\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Duplicate Cities\n",
    "indexes = list(set(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the cities in the top 25 cities that are not in any of  the top destinations\n",
    "filtered_urls = []\n",
    "for i, url in enumerate(other_urls):\n",
    "    if i not in indexes:\n",
    "        filtered_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = DataFrame(filtered_urls) # DF with the information Above\n",
    "df_cities.to_excel(\"Urls/Other Cities URLs.xlsx\", index=False) # Saving the DF to a Xlsx file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scrapping the information of the Other Cities not in the Tope Destinations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_2 = read_excel(\"Urls/Other Cities URLs.xlsx\")\n",
    "dic_cities_2 = df_read_2.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.booking.com/searchresults.en-us.html?label=gen173nr-1FCAEoggI46AdIM1gEaLsBiAEBmAExuAEXyAEM2AEB6AEB-AECiAIBqAIDuAKc7NCiBsACAdICJDYwM2Y1YTI5LTQ2ZjEtNDMzMy1hNWYzLTU0NTc1MDhlMmEzN9gCBeACAQ&aid=304142&ss=Portugal&ssne=Portugal&ssne_untouched=Portugal&lang=en-us&src=searchresults&dest_id=171&dest_type=country&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&nflt=top_destinations%3D%3Buf%3D-&offset=\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Process below is exactly the same as the one described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_17112\\1467691309.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  city_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n"
     ]
    }
   ],
   "source": [
    "# First Part we will get the number of pages per city we have available\n",
    "city_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options) # Loading the Driver\n",
    "for city in dic_cities_2:\n",
    "    city_driver.get(city[\"Url\"])\n",
    "    time.sleep(5) # allow the page to fully load up.\n",
    "    city_page_source = city_driver.page_source # Get the Page Source to parse it with BS4\n",
    "    soup_city = BeautifulSoup(city_page_source, \"lxml\") # Create a new BS4 object with the new page source\n",
    "    try:\n",
    "        last_page = soup_city.find(\"div\", class_=\"d7a0553560\").find(\"ol\", class_ = \"a8b500abde\").find_all(\"li\", class_=\"f32a99c8d1\")[-1].find(\"button\").get(\"aria-label\").strip() # This gets the number of the last page.\n",
    "        city_places = []\n",
    "        for page in list(range(0,int(last_page)*25,25)):\n",
    "            city_driver.get(city[\"Url\"]+\"&offset=\"+str(page))\n",
    "            time.sleep(5)\n",
    "            city_page_source = city_driver.page_source\n",
    "            soup_city = BeautifulSoup(city_page_source, \"lxml\")\n",
    "            houses = []\n",
    "            for house in soup_city.find_all(\"div\", class_ = \"a826ba81c4 fe821aea6c fa2f36ad22 afd256fc79 d08f526e0d ed11e24d01 ef9845d4b3 da89aeb942\"):\n",
    "                place = {}\n",
    "                place[\"Top_destination\"] = city[\"Top Destination\"]\n",
    "                place[\"City\"] = city[\"City\"]\n",
    "                place_2 = get_summary_info(house)\n",
    "                final_place = place | place_2\n",
    "                houses.append(final_place)\n",
    "            city_places.extend(houses)\n",
    "        df_cityplaces = DataFrame(city_places)\n",
    "        df_cityplaces.to_excel(f\"Scraped Data/Top- {city['Top Destination']} City- {city['City']}.xlsx\", index=False)\n",
    "    except:\n",
    "        city_places = []\n",
    "        city_driver.get(city[\"Url\"]+\"&offset=\"+str(0))\n",
    "        time.sleep(5)\n",
    "        soup_city = BeautifulSoup(city_page_source, \"lxml\")\n",
    "        houses = []\n",
    "        for house in soup_city.find_all(\"div\", class_ = \"a826ba81c4 fe821aea6c fa2f36ad22 afd256fc79 d08f526e0d ed11e24d01 ef9845d4b3 da89aeb942\"):\n",
    "            place = {}\n",
    "            place[\"Top_destination\"] = city[\"Top Destination\"]\n",
    "            place[\"City\"] = city[\"City\"]\n",
    "            place_2 = get_summary_info(house)\n",
    "            final_place = place | place_2\n",
    "            houses.append(final_place)\n",
    "            city_places.extend(houses)\n",
    "        df_cityplaces = DataFrame(city_places)\n",
    "        df_cityplaces.to_excel(f\"Scraped Data/Top- {city['Top Destination']} City- {city['City']}.xlsx\", index=False)\n",
    "city_driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Merging all the saved XLSX into one Data Frame ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note ###\n",
    "\n",
    "Steps 3 to 9 can be skipped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have all the summary data, we can noe go to each of the urls and get a more detailed data about the property.<br>\n",
    "First we will read all the XLSX file into a single python Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Scraped Data\"\n",
    "files = glob.glob(path + \"/*.xlsx\") \n",
    "\n",
    "df_list = (read_excel(file) for file in files) # Read all the xlsx file the the folder\n",
    "big_df = concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29174, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_destination</th>\n",
       "      <th>City</th>\n",
       "      <th>Name</th>\n",
       "      <th>URL</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Score</th>\n",
       "      <th>Number_Reviews</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alentejo</td>\n",
       "      <td>Alcácer do Sal</td>\n",
       "      <td>Hotel da Barrosinha</td>\n",
       "      <td>https://www.booking.com/hotel/pt/albergaria-da...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,385 reviews</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alentejo</td>\n",
       "      <td>Alcácer do Sal</td>\n",
       "      <td>Alcácer 4Ever Palace</td>\n",
       "      <td>https://www.booking.com/hotel/pt/alcacer-4-eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304 reviews</td>\n",
       "      <td>Very Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top_destination            City                  Name  \\\n",
       "0        Alentejo  Alcácer do Sal   Hotel da Barrosinha   \n",
       "1        Alentejo  Alcácer do Sal  Alcácer 4Ever Palace   \n",
       "\n",
       "                                                 URL      Rating  \\\n",
       "0  https://www.booking.com/hotel/pt/albergaria-da...  4 out of 5   \n",
       "1  https://www.booking.com/hotel/pt/alcacer-4-eve...         NaN   \n",
       "\n",
       "   Review_Score Number_Reviews Classification  \n",
       "0           8.7  1,385 reviews      Excellent  \n",
       "1           8.0    304 reviews      Very Good  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some properties can appear on multiple cities, we will remove the duplicates based on their URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = big_df[big_df['URL'].duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dics = big_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24910, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Scraping more detailed information using each accomodation URL ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the URL of almost 25K accommodations, we will go to that URL and get more detailed information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function:** get_hotel_info(soup) ###\n",
    "\n",
    "The function below will retrives the Name all the features listed below off of the accomodations Booking Webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hotel_info(soup):\n",
    "    hotel ={}\n",
    "    try:\n",
    "        hotel[\"Accommodation Type\"] = soup.find(\"span\", attrs={\"data-testid\": \"property-type-badge\"}).text # Get the Type of accomodation\n",
    "    except:\n",
    "        hotel[\"Accommodation Type\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Rating Stars\"] = len(soup.find(\"span\", attrs={\"data-testid\": \"rating-stars\"}).find_all(\"span\")) # Get the number of stars. only for hotels\n",
    "    except:\n",
    "        hotel[\"Rating Stars\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Rating Squares\"] = len(soup.find(\"span\", attrs={\"data-testid\": \"rating-squares\"}).find_all(\"span\")) # gets the number of squares, only for non hotels\n",
    "    except:\n",
    "        hotel[\"Rating Squares\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Sustainable Level\"] = soup.find(\"span\", attrs={\"class\": \"d8eab2cf7f cf9ebde7b2 be09c104ad\"}).text # gets the sustainability level.\n",
    "    except:\n",
    "        hotel[\"Sustainable Level\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Address\"] = soup.find(\"span\", attrs={\"class\": \"hp_address_subtitle js-hp_address_subtitle jq_tooltip\"}).text # Gets the address\n",
    "    except:\n",
    "        hotel[\"Address\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Coordinates\"] = soup.find(\"a\", attrs={\"id\": \"hotel_header\"}).get(\"data-atlas-latlng\") # Gets the coordinates\n",
    "    except:\n",
    "        hotel[\"Coordinates\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Property Highlights\"] = soup.find(\"span\", attrs={\"class\": \"ph-item-copy-heart\"}).text # Gets the property highlights\n",
    "    except:\n",
    "        hotel[\"Property Highlights\"] = \"\"\n",
    "    try:\n",
    "        hotel[\"Description\"] = soup.find(\"div\", attrs={\"id\": \"property_description_content\"}).text #Gets the decription of the accomodation\n",
    "    except:\n",
    "        hotel[\"Description\"] = \"\"\n",
    "    try:\n",
    "        features = soup.find(\"div\", attrs={\"data-testid\": \"facility-list-most-popular-facilities\"}).find_all(\"div\") # Gets the most popular features/facilities\n",
    "        for feature in features:\n",
    "            hotel[feature.find(\"span\", attrs={\"class\": \"db312485ba\"}).text] = 1 # Creates a dict, that if the accomodation has an ammenitie, it creates a dict. Key \"name of the amenitie\" Value = 1\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        categories = soup.find(\"div\", attrs={\"class\", \"a1b3f50dcd bdf0df2d01 bfcff4d0ca b2fe1a41c3 c1b465858f d46673fe81\"}).find_all(\"div\", class_ = \"ccff2b4c43 cfc0860887\") # Gets the rating of the categories\n",
    "        for category in categories:\n",
    "            hotel[category.find(\"span\", attrs={\"class\", \"d6d4671780\"}).text] =  category.find(\"div\", attrs={\"class\": \"ee746850b6 b8eef6afe1\"}).text\n",
    "    except:\n",
    "        None\n",
    "    return hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_11916\\1345040833.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  hotel_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n"
     ]
    }
   ],
   "source": [
    "hotel_driver = webdriver.Chrome(\"Driver/chromedriver.exe\", options=options)\n",
    "final_info = []\n",
    "for index, dic in enumerate(final_dics):\n",
    "    hotel_driver.get(dic[\"URL\"])\n",
    "    time.sleep(3) # Load Page\n",
    "    hotel_source = hotel_driver.page_source # get page source\n",
    "    hotel_soup = BeautifulSoup(hotel_source, \"lxml\") #parse page source\n",
    "    dic1 = {\"Name\": dic[\"Name\"], \"Top Destination\": dic[\"Top_destination\"], \"City\": dic[\"City\"], \"URL\": dic[\"URL\"], \"Review_Score\": dic[\"Review_Score\"], \"Number_Reviews\": dic[\"Number_Reviews\"], \"Classification\": dic[\"Classification\"]} # Create Dict with already extracted and saved data\n",
    "    dic2 = get_hotel_info(hotel_soup) # apply function described above\n",
    "    comb_dic = dic1 | dic2 # Merge the 2 Dict\n",
    "    final_info.append(comb_dic)\n",
    "    if index % 500 == 0 or index == len(final_dics) - 1: # This will save the scraped information every 500 accomodations or until it reached the last one. In case IP gets block or some error occurs.\n",
    "        DataFrame(final_info).to_excel(f\"Final Information/Final Data Index - {index}.xlsx\", index=False)\n",
    "hotel_driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
